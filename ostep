Непопадание в TLB обрабатывает либо оборудование, либо программа (ОС). В стародавние времена оборудование распологало сложными наборами команд (CISC). Поэтму оборудование обрабатывало непопадание в кеш полностью самостоятельно. Для этого оборудование должно было точно знать, где в памяти находятся таблиц страниц (для чего использовался регистр базы таблицы страниц), а так же их точный формат. Примером такой устаревшей архитектуры с аппаратно управляемым TLB является архитектура Intel x86, в которой применяется фиксированная многоуровневая таблица страниц, а на текущую таблицу страниц указывает регистр CR3. В более современных архитектурых (например, MIPS R10k), то и другое RISC-компьютеры с сокращенны набором команд) имеется программно управляемый TLB. В случае непопадание в TLB оборудование просто возбуждает исключение, в результате текущий поток команд приостанавливается, уровень привелегий повышается до уровня ядра, и производится переход к обработчику прерывания. Легко догадаться, что этот обработчик - часть ОС, написанной специально с целюб обработки непопаданий в кеш.

Обратите внимание на две детали: первое - после возвращения из обработчика программа должна проддолжить выполнение с ТОЙ ЖЕ команды, которая вызвала прерывание; второе - при выполнении кода обработки непопадания в TLB ОС должна очень внимательно следить за тем, чтобы не вызвать бесконечной цепочки таких непопаданий. 

Главное достоинство программно управляемого решения - в его гибкости. ОС может использоваьб любую структуру данных, которую сочтет выгодной для реализации таблицы страниц, не требуя переделывать оборудование. Другое достоинство - простота.

Типичный TLB может хранить 32, 64 или 128 элементов и быть полностью ассоциативным. По сути дела, это просто означает, что любая конкретная трансляция может находится в любом месте TLB, а оборудование просматривает весь TLB паралельно в поисках искомого. Заметим, что в каждой записи присутствует и VPN, и PFN, поскольку трансляция может оказаться в любом месте TLB (это и называется полной ассоциативностью. Оборудование ищет совпадение во всех записях параллельно.Отступление: почему системный вызов выглядит как вызов функции

Вам, наверное, интересно, почему системный вызов, например open() или read(), выглядит
в точности как вызов типичной функции на C. Точнее, если он выглядит как вызов функции,
то откуда система знает, что это системный вызов, и выполняет все положенные действия?
Ответ прост: это и есть вызов функции, но внутри нее находится команда системного
прерывания. Когда мы вызываем, например, open(), производится обращение к библио-
течной функции. А уже сама библиотека следует принятому соглашению о вызове ядра:
помещает аргументы в точно определенные места (например, в стек или в конкретные
регистры), записывает номер системного вызова в еще одно точно определенное место
(снова в стек или в регистр) и выполняет команду системного прерывания. Библиотечный
код, находящийся после команды прерывания, распаковывает возвращенные значения
и возвращает управление программе, выполнившей системный вызов. Части библиоте-
ки, отвечающие за системные вызовы, написаны на ассемблере, поскольку должны точ-
но следовать соглашению о передаче аргументов и обработке возвращенных значений
и вызывать аппаратную команду прерывания – на C этот код написать невозможно. Теперь
вы знаете, что лично вам не придется писать ассемблерный код для вызова ядра, это уже
сделал кто-то другой.

Для этого ядро на этапе начальной загрузки инициализирует таблицу
прерываний. В процессе начальной загрузки система работает в привиле-
гированном режиме ядра, поэтому может конфигурировать оборудование
как угодно. Едва ли не первым делом ОС устанавливает, что должно делать
оборудование при возникновении определенных исключительных событий.
Например, какой код выполнять, когда происходит прерывание по обраще-
нию к жесткому диску, прерывание от клавиатуры или системный вызов?
ОС информирует оборудование об адресах обработчиков прерываний, для
чего обычно предусмотрена специальная команда. Переданные оборудова-
нию адреса не меняются до следующей перезагрузки машины, поэтому обо-
рудование знает, что делать (к какому коду переходить) в случае системных
вызовов и других исключительных событий.
Обычно каждому системному вызову присваивается уникальный номер
системного вызова. Таким образом, пользовательский код должен помес­
тить номер нужного системного вызова в регистр или в определенную по-
зицию стека, а ОС в ходе обработки извлекает этот номер, проверяет его
и выполняет соответствующий код. Эта косвенность является формой защи-
ты; пользовательский код не может задать точный адрес перехода, а должен
запрашивать обслуживание по номеру.
И последнее замечание: уведомление оборудования о месте нахождения
таблицы прерываний – чрезвычайно опасная операция. Поэтому, как вы
наверняка догадались, она относится к числу привилегированных. Обо-
рудование пресечет попытку выполнить ее в режиме пользователя, и, надо
полагать, вы понимаете, что за этим последует (подсказка: прощай, нехоро-
шая программа). Тема для размышления: что страшного могло бы произойти
с системой, если бы вам было разрешено устанавливать собственную таблицу
прерываний? Могли бы перехватить управление машиной?

Добавление прерывания от таймера позволяет ОС вернуть себе контроль над CPU, даже
если процессы отказываются сотрудничать. Таким образом, это аппаратное средство
принципиально необходимо для контроля над машиной со стороны ОС.

Итак, ОС вернула себе контроль – не важно, кооперативно или силой с по­
мощью прерывания от таймера

Благодаря переключению стеков ядро входит в код переключения в контексте одного процесса (того, что был прерван), а возвращается уже в контексте другого (того, который вот-вот начнет выполняться). Когда ОС наконец выполнит команду возврата из прерывания, новый процесс станет текущим. И на этом контекстное пере-ключение завершается


B протоколе ограниченного прямого выполненения (прерывание от таймера) присутствуют два вида сохранения и вос-становления регистров. Первый – когда происходит прерывание от таймера; в этом случае пользовательские регистры работающего процесса неявно со-храняются оборудованием в стеке ядра данного процесса. Второй – когда ОС решает переключиться с A на B; тогда ядерные регистры явно сохраняются программой (т. е. ОС), но на этот раз в записи о процессе. Последнее действие переводит систему из состояния, когда она вошла в ядро из A, в состояние, когда кажется, что она только что вошла в ядро из B.

Типичные пользовательские приложения работают в режиме пользователя и исполь-зуют системные вызовы, реализованные с помощью системных прерываний, чтобы запросить у операционной системы какие-то услуги

Команда системного прерывания сохраняет состояние регистров, входит в режим ядра и переходит по адресу, хранящемуся в заранее инициализированной таблице пре-рываний.

Когда программа запущена, ОС должна задействовать аппаратные механизмы, чтобы не дать программе работать вечно, а именно прерывание от таймера. Это пример не-кооперативного подхода к планированию CPU.

Политика ОС - механизм принятия решений внутри ОС.

Помимо предположений о рабочей нагрузке, нам для сравнения политик планирования понадобится еще одна вещь: метрика планирования. Мет-рика – это нечто такое, что используется для измерения чего-либо; для оцен-ки качества планирования применяются разные метрики.

Оборотное время - метрика, которая определена как время завершения задания минус время поступления его в систему. (минус - эффект сопровождения)

Еще одна метрика - справдливость.

Политика планирования - Shortest Job First

Невытесняющий планировщик - планировщик, дающий каждому заданию доработать до конца

Планировщик, добавляющий вытеснение в SJF - Shortest Time to Completion First (STCF)

С добавлением интерактивности в работу с ПК оборотное время стало неэффективной метрикой эффективности планирования и появилась новая метрика - время отклика (время поступления - время первого планирования)

Как вы понимаете, STCF и родственные политики не особенно хороши для минимизации времени отклика. Если три задания поступают в одно время, то, например, третье задание должно будет ждать, пока два других завершат-ся полностью, и лишь потом может рассчитывать на получение процессора. С точки зрения оборотного времени, это отличный подход, но в интерактив-ном окружении, когда требуется минимизировать время отклика, он никуда не годится. Действительно, представьте, что вы сидите за терминалом, ввели какую-то команду и должны 10 секунд ждать ответа от системы только по-тому, что какое-то другое задание было запланировано раньше, – приятного мало

Новый алгоритм оптимизации времени отклика - циклический алгоритм (Round Robin), суть которого в том, чтобы давать процессору владеть процессом в течении заданного временного кванта.

Справедливая политика планирования - это политика, которая поровну распределяет процессорное время между активными процессами в локальном временном масштабе

Очевидно, что планировщик должен принять какое-то решение, когда те-кущее задание инициирует запрос ввода-вывода, поскольку во время ожида-ния завершения оно не будет использовать CPU – оно заблокировано. Если речь идет о дисковом вводе-выводе, то блокировка может продолжаться не-сколько миллисекунд или даже дольше в зависимости от текущей загружен-ности диска. Поэтому планировщик, вероятно, должен на это время отдать процессор другому заданию.Также планировщик должен решить, что делать, когда ввод-вывод завер-шится. В этот момент генерируется прерывание, ОС получает управление и переводит процесс, инициировавший ввод-вывод из состояния «забло-кирован» в состояние «готов». Конечно, она могла бы в этот момент заодно запустить это задание. Как ОС должна обращаться с заданиями?

Подход к планированию - многоуровневая аналитическая очередь

Надеемся, что из этого примера вы уяснили одну из главных целей ал-горитма: поскольку он не знает, будет ли задание коротким или долгим, то сначала предполагает, что задание короткое, и назначает ему высокий прио ритет. Если задание действительно короткое, то оно быстро выполнится и завершится, а если нет, то оно будет медленно опускаться вниз и вскоре проявит свою истинную сущность – долго работающее задание, больше на-поминающее пакетный процесс. Именно так MLFQ аппроксимирует SJF.

C целью препятствования переигрывания планировщика будем по истечении квоты для каждого процесса опускать его на уровень ниже

Есть системы, допускающие вмешательство пользователя в задание приоритетов; например, коммандная утилита nice позволяет повышать или понижать приотритет задания, а стало быть, его шансы на получение процессора.

Другой тип планирования - пропорциональный, или равномерный, планировщик. Идея проста: вместо оптимизации оборотного времени или времени отклика планировщик может попытаться гарантировать, что каждое задание получает определенную долю процессорного времени.

Один из способов реализации равномерного планировщика - это лотерейное планирование.

Для решение проблемы неточности лотерейного планирования Вальдспургером было предложено шаговое планирование - детерменированный равномерный планировщик. Для каждого задания определен шаг, обратно пропорциональный количеству принадлежащих ему билетов. Каждый раз, когда некоторый процесс работает, мы будем увеличивать для него счетчик (называемый прогрессом) на величину шага. Каждый раз будет запланировано задание с меньшим прогрессом.

Простая для использования абстракция физической памяти - адресное пространство.

Виртуализация памяти осуществляется как с помощью поддержки оборудование (оно предоставляет для этого эффективный механизм), так и самой ОС, которая должна вмешиваться в стратегически важных точках, чтобы настроить оборудование, т.е она должна управлять памятью, запоминать, какие участки свободны, а какие используются, и рассудительно руководить, чтобы сохранить контроль над использованием памяти. 
Первая реализация аппаратной трансляции адресов - база и граница, или динамическое перемещение. ( физический адрес = виртуальный адрес + база ).

Преобразование виртуального адреса в физический и называется трансляцией адреса - оборудование принимает виртуальный адрес, к которому обращается процесс, и преобразует его в физический адрес, где в действительности находятся данные. Поскольку перемещение производится на этапе выполнения и адресное пространство процесса можно перемещать, даже после того, как он начал выполняться, этот метод называется динамическим перемещением.

Регистр границы служит для защиты.

Отметим, что регистры базы и границы - это аппаратные конструкции, находящиеся на кристалле (по одной паре на каждый процессор). Иногда часть процессора, занимающуюся трансляцией адресов, называют устройствой управления памятью (memory management unit - MMU).

Прерывание - это событие, которое изменяет нормальный поток выполнения программы.
Исключения - это прерывания, генерируемые процессором, когда CPU обнаруживает ошибку, например деление на ноль и т.д.

С введением такого способа виртуализации памяти добавляются два новых регистра для контекстного переключения. ОС также должна сохранять их для каждого процесса. в блоке управления процессом.

Обратите внимание, что трансляция адресов производится оборудованием без участия ОС.

Трансляция адресов - это механизм виртуализации памяти.

Недостаток такого метода - неэффективной расход памяти. На картинке было видно, что куча и стек малы и между ними есть свободное пространтво. Эта память расходуется впустую. Такие бесполезные потери обычно называются внутренней фрагментацией.

Первой попыткой исправления этого недостатка было обобщение подхода на основе базы и границы, называемое сегментацией. Смысл ее в том, чтобы вместо одной пары регистров базы и границы в MMU завести по паре для каждого логического сегмента адресного пространства.

Термин "ошибка сегментации" (segmentation fault) означал попытку доступа к недопустимому адресу на машине с сегментированной паматью. Смешно, но термин прижился даже для машин, не поддерживающих сегментацию. 

Но как аппаратура понимает к какому сегменту происходит обращение. Один из распространенных подходов, называемый явным - разбивать адресное пространство на сегменты, анализируя несколько старших битов виртуального адреса. (VAX/VMS)

Существуют и другие способы аппаратного определения сегмента, которому принадлежит адрес. В случае неявного подхода оборудование определяет сегмент, анализируя, как сформирован адрес. 

Но что насчет стека, как реализовать идею базы и границы на нем, ведь он растет в другую сторонуи трансляция должна выполняться по другому. Здесь нужна помощь со стороны оборудования и помимо регистров базы и границы оборудование должно еще и знать в какую сторону растет сегмент (бит).

Для экономии памяти иногда полезно разделять некоторые сегменты. Разработчики поняли, что это повысит эффективность использования памяти, если совсем немного рпсширить аппаратную поддержку. (Разделение кода применятеся в системах и по сей день, позволяя отобразить один и тот же сегмент физической памяти на несколько виртуальных адресных пространств). Для реализации разделения нужна дополнительная аппаратная поддержка в виде битов защиты.

Сегментация поднимает ряд новых вопросов. Одна из них - управление свободным пространством. Из за обилия сегментов в памяти очень скоро образуются "дыры" свободного пространства, которые не дают возможности выделить место для новых сегментов или увеличить размер существущих. Эта проблема называется внешней сегментацией.

Одно из решений - уплотнение физической памяти. Но это слишком дорого, поэтому проще написать алгоритмы для управления списком свободных, которых буквально сотни.

Страничная организация заключается в делении памяти на блоки фиксированного размера.

Указатель без конкретизации типа - указатель на void.

Библиотека для управления кучей пользовательского процесса - malloc.

Распределители памяти могут сталкиваться с проблемой внутренней фрагментации; если распределитель возвращает блоки, большие, чем запрашивалось, то избыточное пространство в таком блоке считается внутренней фрагментацией.

При реализации распределителей памяти можно использовать список свободных. Если запрошен участок памяти точно удовлетворяющий запросу, то мы просто возвращаем его и убираем из списка. Но что делать, если пользователь запросил участок памяти меньше, чем есть? В этом случае распределитель выполнит операцию разделения на 2. Таким образом, разделение используется, когда размер запрашиваемого блока меньше размера доступного свободного блока.

Парный механизм, присутствующий во многих распределителях, называется объединением свободного пространства. Идея следующая: после освобождения, соединять соседние свободные блоки до тех, пока такие есть.

Для определения размера выделенного блока (например, при освобождении его с помощью free()) помимо самого блока выделяется память для специального заголовка, которые, помимо другой информации, содержит размер выделенного блока. (обычно он располагается непосредственно перед выделяемым блоком).

Последний механизм, который присутствует во многих библиотеках работы с паматью: что делать, если в куче не осталось места? Проще всего вернуть ошибку... В большинстве традиционных распределителей памяти начальный размер кучи невелик, а затем по мере необходимости запрашивается дополнительная память у ОС. Обычно для этого выполняется тот или иной системный вызов (в большинстве UNIX-систем sbrk), после чего начинается выделение новых блоков оттуда.

Идеальный распределитель должен быть быстрым и минимизировать фрагментацию.

Стратегии управления свободным пространством:
1) лучший подходящий
2) худший подходящий (найти наибольший блок и вернуть из него запрошенный объем памяти, а оставшийся (большой) блок оставить в списке свободных. Смысл в том, чтобы оставлять свободными большие блоки, а не множество мелких, как в подходе лучший подходящий.
3) первый подходящий (иногда начало списка свободных засоряется мелкими объектами)
4) следующий подходящий (начать поиск с позиции, где закончился предыдущий)
5) сегрегированные списки. основаная идея в том, что если некоторое приложение особенно часо запрашивает блоки какого то одного (или нескольких) размера, то следует завести отдельный список для объектов этого размера, а все остальные запросы переадресовывать общему распределителю памяти.
6) поскольку объединение - критически важная часть распределителя, было предложено несколько подходов, упрощающих именно этот аспект. один из них - метод близнецов. Причина такой хорошей работы метода близнецов - простота нахождения близнеца данного блока.

Одна из главных проблем многих описанных выше подходов - недостаточная масштабируемость. Поэтому в передовых распределителях используются более сложные структуры данных.

При решении большинства проблем управления пространством операционная система принимает один из двух подходов: первый - нарезать на куски переменного размера (сегментация); второй - нарезка пространства на блоки фиксированного размера. В подсистеме виртуальной памяти эта идея называется страничной организацией (участок - страница). При этом физическая память предствляется в виде массива слотов фиксированного размера, называемых страничными рамками. При этом каждая страничная рамка может содержать одну страницу виртуальной памяти.

Чтобы запомнить, в каком месте физической памяти находится каждая виртуальная страница адресного пространства, ОС обычно хранит для каждого процесса структуру данных, называемую таблицей страниц. Ее основная роль - хранить трансляции адресов для каждой виртуальной страницы.

Для трансляции виртуального адреса, сгенерированного процессом, мы должны сначала выделить из него 2 компоненты: номер виртуальной страницы (VPN) и смещение от начала этой страницы. Зная номер виртуальной страницы , мы можем найти соответствующую запись в таблице страниц и определить , в какой физической рамке находится эта виртуальная страница. Номер физической рамки - PFN, иногда называемый также номером физической страницы (PPN).

Заметим, что после трансляции смещение не изменяется (т.е оно не транслируется)

Запись таблицы страниц - PTE.

Поскольку таблицы страниц могут быть просто огромными, мы не заводим в аппаратном устройстве управления памятью на кристалле места для хранения таблицы страниц текущего процесса. Вместо этого таблицы страниц для каждого процесса нужно хранить где то в памяти.

Теперь поговорим об устройстве таблицы страниц. Это просто структура данных, используемая для отображения виртуальных адресов (точнее, номеров виртуальных страниц) в физические (номера страничных рамок (физических страниц)). 

Простейшей форма таблицы страниц называется линейной таблицей страниц, это обычный массив. ОС ищет в этом массиве запись таблицы страниц (PTE) по индексу, равному номеру виртуальной страницы (VPN), и выбирает из этой записи номер физической рамки (PFN).

В каждой PTE храниться несколько битов, заслуживающих внимания. Бит достоверности (valid bit) показывает, действует ли данная трансляция (стр. 231). Используется для поддержки разряженного пространства: просто помечая неиспользуемые страницы как недействительные, мы устраняем необходиость выделять для них физические рамки и тем самым экономим уйму памяти. Биты защиты показывают, что можно делать со страницей: читать, записывать или выполнять. Кстати, при нарушении доступа будет сгенерировано системное прерываение (это касается всех битов до и после этой записи). Бит присутствия показывает, где находится данная страница: в физической памяти или на диске (т.е выгружена). Бит изменения (dirty bit) говорит, что страница была изменена с момента ее загрузки в память. Бит обращения (или бит доступа) иногда используется, чтобы показать, что к странице было обращение. Это полезно, когда нужно определить, какие страницы популярны, так что лучше их оставить в памяти; знать об этом необходимо для вытестения страниц.

Логично, что для трансляции виртуального адреса в физический оборудование должно знать адрес начала таблицы страни процесса.

Недостатком простого хранения таблицы страниц в памяти ОС является то, что при каждом обращении к памяти (будь то выборка команды или явная загрузка или сохранение) от нас требуется выполнить одну дополнительную операцию доступа к памяти, чтобы выбрать данные о трансляции из таблицы страниц. Это очень много работы!

Во время работы программы каждая выбора команды генерирует два обращения к памяти: одно к таблице страниц для поиска физической страницы, на котором находится команда, а второе к самой команде, чтобы передать ее процессору для выполнения. Кроме того, имеется одно явное обращение к памяти в команде mov; оно влечет за собой второй доступ к таблице страниц (чтобы транслировать виртуальный адрес в физический) и затем доступ к самому массиву.

При реализации страничной организации нужно проявлять осторожность, так как при этом замедляется работа машины (появляется много дополнительных обращений к памяти, необходимых для доступа к таблицам страниц) и потребляется очень много памяти (заполненной таблицами страниц, а не полезными данными). Поэтому необходимо еще подумать и изобрести страничную систему, которая не просто работает, а работает хорошо.

Чтобы ускорить трансляцию адресов, мы добавим так называемый (в силу исторических причин) буфер ассоциативной трансляции (translation-lookaside buffer - TLB). TLB - часть устройства управления памятью (MMU), расположенного на кристале, и по сути дела является аппаратным кешем часто повторяющихся трансляций виртуального адреса в физический, поэтому лучше было назвать его кешем трансляции адресов. При каждом обращении к виртуальной памяти оборудование сначала смотрит, нет ли нужной трансляции в TLB; если есть, то трансляция производится быстро без необходимости заглядывать в таблицу страниц (где храняться все трансляции). Из за своего огромного влияния на производительность TLB в самом деле в буквальном смысле превращает идею виртуальной памяти в реальность.

TLB, как и все кеши, основан на допущении, что в типичном случае трансляция будет найдена в кеше. Если так, то накладные расходы невелики, поскольку TLB расположен рядом с процессорным ядром и спроектирован так, что работает очень быстро.

Даже при первом проходе этой программы по массиву TLB повышает эффективность благодаря пространственной локальности. Элементы массива плотно упакованы на страницах (т.е близки друг к другу в пространстве), поэтому только первое обращение к элементу на странице заканчиваетс непопаданием в TLB. Отметим также роль размера страницы в этом примере (стр. 243). Если бы размер страницы был всего в два раза больше (32 байта вместо 16), то количество непопаданий при доступе к массиву еще уменьшилось бы. Поскольку типичные размеры страниц гораздо больше 4кб, эффективность доступа к таким плотным массивам очень высока - всего одно непопадание на целую страницу элементов. И последнее замечание относительно производительности TLB: если вскоре после выхода из цикла программа снова обратиться к этому массиву, то результат, скорее всего, будет еще в предположении, что TLB достаточно велик, чтобы кешировать все трансляции. В этом случае коэффициент попаданий в TLB был бы велик вследствии временной локальности, т.е близкого соседства обращений к одним и тем же элементам во времени. Как и для любого кеша, успех применения TLB зависит от пространственной и временной локальности, а это свойства программы.

Существует два типа локальности: временная и пространственная.

Возникает вопрос: если кеши (в частности, TLB) так хороши, то почему бы не изготовить большие кеши и хранить в них все данные? К сожалению, здесь мы сталкиваемся с фундаментальными законами физики. Если вы хотите быстрый кэш, то он должен быть маленьким, поскольку в игру вступают скорость света и другие физические ограничения. Любой большой кеш по определению был бы медленным, так что вся затея потеряла бы смысл.

CISC и RISC - типы процессорной архитектуры.

Непопадание в TLB обрабатывает либо оборудование, либо программа (ОС). В стародавние времена оборудование распологало сложными наборами команд (CISC). Поэтму оборудование обрабатывало непопадание в кеш полностью самостоятельно. Для этого оборудование должно было точно знать, где в памяти находятся таблиц страниц (для чего использовался регистр базы таблицы страниц), а так же их точный формат. Примером такой устаревшей архитектуры с аппаратно управляемым TLB является архитектура Intel x86, в которой применяется фиксированная многоуровневая таблица страниц, а на текущую таблицу страниц указывает регистр CR3. В более современных архитектурых (например, MIPS R10k), то и другое RISC-компьютеры с сокращенны набором команд) имеется программно управляемый TLB. В случае непопадание в TLB оборудование просто возбуждает исключение, в результате текущий поток команд приостанавливается, уровень привелегий повышается до уровня ядра, и производится переход к обработчику прерывания. Легко догадаться, что этот обработчик - часть ОС, написанной специально с целюб обработки непопаданий в кеш.

Обратите внимание на две детали: первое - после возвращения из обработчика программа должна проддолжить выполнение с ТОЙ ЖЕ команды, которая вызвала прерывание; второе - при выполнении кода обработки непопадания в TLB ОС должна очень внимательно следить за тем, чтобы не вызвать бесконечной цепочки таких непопаданий. 

Главное достоинство программно управляемого решения - в его гибкости. ОС может использоваьб любую структуру данных, которую сочтет выгодной для реализации таблицы страниц, не требуя переделывать оборудование. Другое достоинство - простота.

Типичный TLB может хранить 32, 64 или 128 элементов и быть полностью ассоциативным. По сути дела, это просто означает, что любая конкретная трансляция может находится в любом месте TLB, а оборудование просматривает весь TLB паралельно в поисках искомого. Заметим, что в каждой записи присутствует и VPN, и PFN, поскольку трансляция может оказаться в любом месте TLB (это и называется полной ассоциативностью. Оборудование ищет совпадение во всех записях параллельно.

При использовании TLB возникают новые проблемы, связанные с переключением мкжду процессами (а значит, и адресными пространствами). Именно, TLB содержит только трансляции виртуальных адресов в физические, действительные только для текущего процесса; для остальных же процессов они не имеют смысла.

Существо проблемы: как управлять содержимым TLB при контекстном переключении. После контекстного переключения процессов находящиеся в TLB трансляции предыдущего процесса не имеют смысла для последующего. Что оборудование или ОС должны сделать для решения этой проблемы?

Одно из нескольких решений: сброс TLB в начальное состояние. В программно управляемой система этого можно достичь с помощью явной (и привилегированной) команды, а в аппаратно управляемой сброс можно инициировать в момент изменения регистра базы таблицы страниц (PBTR). Операция сброса просто устанавливает все биты достоверности в 0, очищая тем самым TLB от содержимого. Недостаток этого решения очевиден: при каждом возобновлении процесс поначалу будет сталкиваться с непопаданиями.

Чтобы уменьшить накладные расходы, в некоторых системах добавлена аппаратная поддержка TLB после контекстных переключений. В частности, в TLB может присутствовать поле идентификатора адресного пространства (ASID). Можно его считать аналогом идентификатора процесса, но обычно битов в нем меньше. Конечно, оборудованию нужно также знать, какой процесс выполняется в данный момент, чтобы выполнять трансляции для него, поэтому ОС при контекстном переключении должна заносить в некий привилегированный регистр ASID текущего процесса.

При работе с TLB, как и с любым другим кешем, необходимо рассмотреть проблему вытестения из кеша. Именно, помещая в TLB новую запись, мы должны вытеснить какую-то старую - а какую?

Один из самых популярных подходов - вытеснять запись по давности использования (least recently used - LRU). Идея в том, чтобы воспользоваться локальностью ссылок в памяти и предполагать, что запись, которая давно не использовалась, - лучший кандидат на вытеснение. Другой типичный подход - раномизированная политика (существенный плюс - простота).

Запоминающее устройство с произвольной выборкой (ЗУПВ) (англ. random-access memory - RAM) означает, что время обращения к любой части ЗУПВ одинаково. 

Теперь займемся второй проблемой страничной организации: таблицы страниц слишком велики и потому потребляют чересчур много памяти.

Простые таблицы страниц в виде массива (обычно называемые линейными) слишком велики.

Размер таблицы страниц можно было бы уменьшить очень просто: использовать увеличенные страницы. Однако у этого подхода есть серьезная проблема: большие страницы ведут к растранжириванию места внутри каждой страницы, т.е к внутренней фрагментации. Так как приложение выделяет страницы, но использует лишь их малую часть, память быстро забивается такими чрезмерно большими страницами. Поэтому в большинстве систем используются лишь сравнительно малые страницы: 4КБ (как в х86) или 8КБ (как в SPARCv9).

Отметим, что во многих современных архитектурах поддерживаются страницы нескольких размеров. Основная причина использования страниц разных размеров - не столько экономия места для таблицы страниц, сколько уменьшение нагрузки на TLB. Но, как показывают исследования, наличие нескольких размеров страниц заметно усложняет диспетчер виртуальной памяти в ОС, поэтому иногда большие страницы проще всего использовать напрямую, экспортируя приложениям новый интерфейс.

Джеку Деннису при конструировании системы виртуальной памяти Multics пришла мысль объединить страничную организацию и сегментацию, чтобы уменьшить накладные расходы, связанные с таблицами страниц.

Идея это гибридного подхода: вместо того чтобы заводить единственную линейную таблицу страниц для всего адресного пространства процесса, почему бы не завести по одной таблице на каждый логический сегмент.

В системе с сегментацией у нас был регистр базы, который сообщал, в каком месте физической памяти находится сегмент, и регистр границы, содержавший размер сегмента. В гибридной модели эти структуры по прежнему присутствуют внутри MMU, но только база указывает не на сам сегмент, а содержит физический адрес таблицы страниц этого сегмента. А регистр границы обозначает конец таблицы страниц (т.е. сколько в ней действительных страниц).

Чтобы понять, к какому сегменту относиться адрес, мы будем использовать два старших бита адреса. Что касается оборудования, будет предполагать, что имеется три пары регистров базы и границы, по одному для сегментов кода, кучи и стека.

Критически важное отличие нашей гибридной схемы - наличие регистра границы для каждого сегмента. В регистре границы храниться номер максимальной действительной страницы в сегментВ системе с сегментацией у нас был регистр базы, который сообщал, в каком месте физической памяти находится сегмент, и регистр границы, содержавший размер сегмента. В гибридной модели эти структуры по прежнему присутствуют внутри MMU, но только база указывает не на сам сегмент, а содержит физический адрес таблицы страниц этого сегмента. А регистр границы обозначает конец таблицы страниц (т.е. сколько в ней действительных страниц).

Чтобы понять, к какому сегменту относиться адрес, мы будем использовать два старших бита адреса. Что касается оборудования, будет предполагать, что имеется три пары регистров базы и границы, по одному для сегментов кода, кучи и стека.

Критически важное отличие нашей гибридной схемы - наличие регистра границы для каждого сегмента. В регистре границы храниться номер максимальной действительной страницы в сегменте.

Другой подход не опирается на сегментацию, но призван решить ту же проблему: как не хранить в памяти области, занятиые недействительными записями таблицы страниц? Этот подход называется многоуровневой таблицей страниц, поскольку линейная таблица страниц преобразуется в древовидную. Он настолько эффективен, что применяется в большинстве современных систем например, х86). Идея в том, чтобы разбить таблицу страниц на блоки размером со страницу. Затем, если целая страница состоит только из недействительных записей, не станем выделять для нее память. Для отслеживания того, какие страницы действительны (и где они в таком случае находяться в памяти), используется новая структура - каталог страниц. Таким образом, каталог можно использовать для двух целей: узнать, где расположена страница, присутствующая в таблице страниц, и имеется ли в данной странице хотябы одна действительная запись. Многоуровневая таблица страниц исключает части динейной таблицы (освобождая соответствующие физические рамки для других целей) и с помощью каталога следить за тем, какие страницы таблицы страниц находятся в памяти. Каталог страниц - это просто таблица второго уровня, которая содержит по одной записи на каждую страницу таблицы страниц. Он состоит из записей каталога страниц (page directory entry - PDE). PDE содержит (как минимум) бит достоверности и номер физической рамки.

Следует отметить, что у многоуровневых таблиц тоже есть цена: при непопадании в TLB, чтобы получить информацию о трансляции из таблицы страниц, придется обратиться к памяти дважды (к каталогу страниц и к самой записи PTE), а не один раз, как в случае линейной таблицы страниц. Так что многоуровневая таблица - пример компромисса между пространством и временем.

Берегитесь избыточной сложности, будь то в форме преждевременно оптимизируемого кода или в какой то другой, потому что такое решение будет труднее понять, сопровождать и отлаживать.

До сих пор мы предполагали, что в многоуровневой таблице страниц имеется всего 2 уровня. Но в некоторых случаях возможно (и даже необходимо) более глубокое дерево.

Еще большей экономии памяти позволяют добиться инвертированные таблицы страниц. В этом случае вместо нескольких таблиц страниц (по одной на каждый работающий в системе процесс) мы храним одну таблицу страниц, в которой имеется запись для каждой физической страницы в системе. Эта запись сообщает, какой процесс использует данную страницу и какая виртуальная страница этого процесса на нее отображена. Для нахождения нужной записи необходимо произвести поиск в этой структуре данных. Линейный поиск обошелся бы слишком дорого, поэтому часто над базовой структурой надстраивается хеш-таблица.

В случае если таблица страниц не может полностью поместиться на диске, ее часть размещается в виртуальной памяти ядра, что позволяет системе выгружать их на диск, когда памяти не хватает.

До сих пор мы предполагали, что адресное пространство нереалистично маленькое и целиком помещается в физическую память. Более того, мы предполагали, что адресные пространства всех работающих процессов помещаются в память. Теперь мы ослабим эти предположения и обсудим, как поддержать много больших адресных пространств одновременно работающих процессов. Для этого нам понадобиться дополнительный уровень иерархии запоминающих устройств.

Большие адресные пространства нужны для удобства и простоты использования. Располагая большим адресным пространством, мы избавлены от необходимости думать, хватит ли в памяти места для структур данных и т.д, мы можем писать код свободно. Противоположный пример дают старые системы, в которых использовалась оверлейная память, когда программист вынужден был вручную перемещать код и данные в память и из памяти по мере необходимости.

Предположим, что имеется емкое и сравнительно медленное устройство, которым можно воспользоваться, чтобы создать иллюзию очень большой виртуальной памяти, даже большей чем сама физическая память.

Мультипрограммирование - способ организации выполнения нескольких программ на одном компьютере. С приходом мультипрограммирования возникла настоятельная необходимости выгружать часть страниц, потому что ранние компьютеры просто не могли хранить все страницы всех процессов в памяти. Таким образом, желание обеспечить мультипрограммирование вкупе с простотой использования заставляет нас поддерживать иллюзию большей памяти, чем физически доступно. Так поступают все современные системы ВП.

Прежде всего мы должны зарезервировать на диске место для выгрузки страниц. Обычно в операционных системах это место называют областью подкачки. Будем предполагать, что ОС умеет читать область подкачки и записывать в нее блоки размером со страницу. Для этого ОС должна запоминать дисковый адрес данной страницы.

Вспомним, что происходит при обращении к памяти. Выполняемый процесс генерирует адреса виртуальной памяти (для выборки команд и доступа к данным), а оборудование транслирует их в физические адреса перед фактической выборкой из памяти.



Еще большей экономии памяти позволяют добиться инвертированные таблицы страниц. В этом случае вместо нескольких таблиц страниц (по одной на каждый работающий в системе процесс) мы храним одну таблицу страниц, в которой имеется запись для каждой физической страницы в системе. Эта запись сообщает, какой процесс использует данную страницу и какая виртуальная страница этого процесса на нее отображена. Для нахождения нужной записи необходимо произвести поиск в этой структуре данных. Линейный поиск обошелся бы слишком дорого, поэтому часто над базовой структурой надстраивается хеш-таблица.

В случае если таблица страниц не может полностью поместиться на диске, ее часть размещается в виртуальной памяти ядра, что позволяет системе выгружать их на диск, когда памяти не хватает.

До сих пор мы предполагали, что адресное пространство нереалистично маленькое и целиком помещается в физическую память. Более того, мы предполагали, что адресные пространства всех работающих процессов помещаются в память. Теперь мы ослабим эти предположения и обсудим, как поддержать много больших адресных пространств одновременно работающих процессов. Для этого нам понадобиться дополнительный уровень иерархии запоминающих устройств.

Большие адресные пространства нужны для удобства и простоты использования. Располагая большим адресным пространством, мы избавлены от необходимости думать, хватит ли в памяти места для структур данных и т.д, мы можем писать код свободно. Противоположный пример дают старые системы, в которых использовалась оверлейная память, когда программист вынужден был вручную перемещать код и данные в память и из памяти по мере необходимости.

Предположим, что имеется емкое и сравнительно медленное устройство, которым можно воспользоваться, чтобы создать иллюзию очень большой виртуальной памяти, даже большей чем сама физическая память.

Мультипрограммирование - способ организации выполнения нескольких программ на одном компьютере. С приходом мультипрограммирования возникла настоятельная необходимости выгружать часть страниц, потому что ранние компьютеры просто не могли хранить все страницы всех процессов в памяти. Таким образом, желание обеспечить мультипрограммирование вкупе с простотой использования заставляет нас поддерживать иллюзию большей памяти, чем физически доступно. Так поступают все современные системы ВП.

Прежде всего мы должны зарезервировать на диске место для выгрузки страниц. Обычно в операционных системах это место называют областью подкачки. Будем предполагать, что ОС умеет читать область подкачки и записывать в нее блоки размером со страницу. Для этого ОС должна запоминать дисковый адрес данной страницы.

Вспомним, что происходит при обращении к памяти. Выполняемый процесс генерирует адреса виртуальной памяти (для выборки команд и доступа к данным), а оборудование транслирует их в физические адреса перед фактической выборкой из памяти.



Еще большей экономии памяти позволяют добиться инвертированные таблицы страниц. В этом случае вместо нескольких таблиц страниц (по одной на каждый работающий в системе процесс) мы храним одну таблицу страниц, в которой имеется запись для каждой физической страницы в системе. Эта запись сообщает, какой процесс использует данную страницу и какая виртуальная страница этого процесса на нее отображена. Для нахождения нужной записи необходимо произвести поиск в этой структуре данных. Линейный поиск обошелся бы слишком дорого, поэтому часто над базовой структурой надстраивается хеш-таблица.

В случае если таблица страниц не может полностью поместиться на диске, ее часть размещается в виртуальной памяти ядра, что позволяет системе выгружать их на диск, когда памяти не хватает.

До сих пор мы предполагали, что адресное пространство нереалистично маленькое и целиком помещается в физическую память. Более того, мы предполагали, что адресные пространства всех работающих процессов помещаются в память. Теперь мы ослабим эти предположения и обсудим, как поддержать много больших адресных пространств одновременно работающих процессов. Для этого нам понадобиться дополнительный уровень иерархии запоминающих устройств.

Большие адресные пространства нужны для удобства и простоты использования. Располагая большим адресным пространством, мы избавлены от необходимости думать, хватит ли в памяти места для структур данных и т.д, мы можем писать код свободно. Противоположный пример дают старые системы, в которых использовалась оверлейная память, когда программист вынужден был вручную перемещать код и данные в память и из памяти по мере необходимости.

Предположим, что имеется емкое и сравнительно медленное устройство, которым можно воспользоваться, чтобы создать иллюзию очень большой виртуальной памяти, даже большей чем сама физическая память.

Мультипрограммирование - способ организации выполнения нескольких программ на одном компьютере. С приходом мультипрограммирования возникла настоятельная необходимости выгружать часть страниц, потому что ранние компьютеры просто не могли хранить все страницы всех процессов в памяти. Таким образом, желание обеспечить мультипрограммирование вкупе с простотой использования заставляет нас поддерживать иллюзию большей памяти, чем физически доступно. Так поступают все современные системы ВП.

Прежде всего мы должны зарезервировать на диске место для выгрузки страниц. Обычно в операционных системах это место называют областью подкачки. Будем предполагать, что ОС умеет читать область подкачки и записывать в нее блоки размером со страницу. Для этого ОС должна запоминать дисковый адрес данной страницы.

Вспомним, что происходит при обращении к памяти. Выполняемый процесс генерирует адреса виртуальной памяти (для выборки команд и доступа к данным), а оборудование транслирует их в физические адреса перед фактической выборкой из памяти.

Для определения того, находится ли страница, на которую указывает PTE, в физической памяти (необходимость этого возникает при реализации выгрузки страницы на диск), оборудование (или ОС в случае программно управляемого TLB) использует новый флаг в записи таблицы страниц - бит присутствия.

Доступ к странице, отсутствующей в физической памяти, называется отказом страницы.

После отказа страницы для его обработки вызывается ОС (посредством исключения). Начинает работать специальный код - обработчик отказа страницы.

В любом случае, если страница отсутствует в памяти, ОС поручается обработать отказ страницы (несмотря на различия в плане работы TLB: программно\аппаратно управляемый). Практически во всех системах отказы страниц обрабатываются программно, даже в системе с аппаратно управляемым TLB оборудование оставляет эту важную работу ОС.

Если страница отсутствует и была выгружена на диск, то в процессе обработки отказа страницы ОС должна подкачать страницу в память. Но каким образом, ОС узнает, где искать нужную страницу? Во многих системах самое подходящее место для хранения такой информации - таблица страниц. Таким образом, ОС могла бы использовать для хранения дискового адреса те биты PTE, в которых обычно находится PFN страницы. Видя отказ страницы, ОС берет адрес из PTE и обращается к диску с запросом загрузить страницу в память.

При необходимости подкачки страницы может не оказаться достаточно свободной физической памяти. В этой случан ОС, возможно, придется выгрузить одну или несколько страниц, чтобы освободить место для новых. Процедура выбора подлежащей выгрузке страницы называется политикой замещения страниц.

Очерчение полного потока управления при доступе к памяти представлено на странице 279. Это может быть прекрасным ответом на вопрос: "Что происходит, когда программа выбирает данные из памяти?".

До сих пор при описании процедуры замещения мы предполагали, что ОС ждет, когда память заполниться, и только тогда замещает (вытесняет) страницу, чтобы освободить место для другой. Но в действительности это не так, и по разным причинам ОС всегда придерживает немного свободной памяти. Для этой цели в большинстве операционных систем определены верхний (high watermark - HW) и нижний пределы, помогающие решить, когда начинать вытеснение страниц из памяти. Работает это так. Когда ОС замечает, что осталось меньше LW свободных страниц, запускается фоновый поток, отвечающий за особождение памяти. Этот поток вытесняет страницы, покв количество свободных не станет равно HW. Фоновый поток, иногда называемый демоном выгрузки, или демоном страничного обмена, затем засыпает, довольный тем, что освободил память, чтобы ей могли воспользоваться работающие процессы и ОС.

Решение о том, какую страницу (или страницы) вытеснить, инкапсулированно в политике замещения.

Учитывая, что в основной памяти находится некоторое подмножество всех страниц в системе, мы можем с полным правом рассматривать ее как кеш для страниц виртуальной памяти. Поэтому наша цель при выборе политики замещения в этом кеше - минимизировать количество непопаданий в кеш, т.е случаев, когда приходится подкачивать страницу с диска.

Всеми силами нужно избегать непопаданий в кеш, иначе программа будет работать медленно - со скоростью диска. Один из способов добиться этой цели - тщательно проектировать политику замещения, чем мы сейчас и займемся.

Лучшая теоретическая политика замещения была разработана Белади много лет назад (он назвал ее MIN). При использовании этой оптимальной политика общее число непопаданий будет наименьшим. Белади показал, что оптимальным является простой (но, к сожалению, трудный для реализации) подход, когда замещается страница, которая не потребуется в будущем дольше всего. Эта политика используется в качестве эталона для сравнения качества работы других политик.

Оптимальная политика послужит нам лишь эталоном для сравнения, чтобы оценить близость к идеалу.

Во многих ранних системах стремились избегать сложности, сопровождающей попытки приблизиться к идеалу, и применяли очень простые политики замещения. Например, использовалась политика FIFO.

Случайная политика замещения просто вытесняет случайную страницу в случае дефицита памяти.

К сожалению, любой политике, настолько простой, как FIFO или случайная, свойтсвенна общая проблема: она может вытеснить важную страницу, к которой возможно обращение в ближайшем будущем. Как и в случае с политикой планирования, чтобы улучшить гипотезу о будущем поведении, обратимся к прошлому и будем опираться на историю. В качестве исторической информации политика замещения страниц могла бы воспользоваться частотой. Другое часто используемое свойство страницы - недавность обращения. Это семейство политик основано на принципе локальности. Так и появилось на свет семейство простых основанных на истории алгоритмов. Политика вытеснения наименее часто используемой (Least Frequently Used - LFU) замещает страницу, которая использовалась наименее часто. А политика вытеснения по давности использования (Least Recently Used - LRU) замещает страницу, которая не использовалась дольше других.

(LRU) Чтобы следить за тем, какие страницы используются чаще всего и реже всего, система должна выполнять какие то действия при каждом обращении к памяти. Очевидно, что без должной аккуратности такой учет обойдется в копеечку.

Некоторого ускорения поможет добиться толика поддержка со стороны оборудования, которое, например, при каждом доступе к странице обновлять некоторое поле времени в памяти. К сожалению, количество страниц в системе растет и поиск гигантского массива (или чего нибудь другого) в поисках страницы, которая не использовалась дольше всех прочих. Это слишком дорого! Нельзя ли обойтись аппроксимацией.

Приближенная LRU более практична с точки зрения накладных расходов, именно так и делается во многих современных системах. Для реализации идеи необходима аппаратная поддержка в форме бита использования для каждой страницы. При каждом обращении этот бит устанавливается в 1. Для апроксимации LRU может быть использован, например, алгоритм часов. Если представить, что все страницы организованы в виде циклического списка и его обход начинается со случайного узла, то первый узел с битом использования 0 и будет вытеснен.

Модификация алгоритма часов - дополнительно учитывать, была ли страница изменена во время нахождения в памяти. Объяснение: если страница была модифицирована, и, значит, является грязной, то при вытеснении ее необходимо записать на диск, вытеснение же чистых страниц ничего не стоит. Для поддержки такого поведения оборудование должно так же устанавливать бит изменения.

Замещение страниц - не единственная политика, применяемая в подсистеме ВП. Например, ОС также должна решать, когда подкачать страницу в память. Это политика выбора страницы. Для большинства страниц используется просто подкачка по запросу. Конечно, ОС могла бы догадаться, что страницу скоро нужно будет использовать, и подкачать заранее; такое поведение называется предвыборкой. Еще одна политика определяет, как ОС выгружает страницы на диск. Во многих системах набирается группа страниц для выгрузки. Такое поведение обычно называют кластеризацией.

Пробуксовка - это ситуация, когда спрос на память со стороны работающих процессов превышает размер доступной физической памяти и система будет постоянно выгружать и подкачивать страницы. 

В Linux при обнаружении серьезного дефицита памяти запускается убийца пожирателей памяти (out of memory killer); этот демон выбирает процесс, который особенно интенсивно потребляет память, и принудительно завершает его, бесцеремонно снижая потребление.

(VAX/VMS) При контекстном переключении изменяет регистры P0 и P1, так чтобы они указывали на таблицы страниц готового к выполенению процесса, но не изменяет регистры базы и границы S, поэтому на каждое адресное пространство пользователя отображаются одни и те же структуры ядра. Ядро отображается в каждое адресное пространство по нескольким причинам. Прежде всего так ядру проще работать; например, получив от пользовательской программы указатель (скажем, при выполнении системного вызова write), ОС может легко скопировать адресуемые этим указателям данные в собственные структуры. ОС естественно пишется и компилируется, т.к не нужно думать о том, откуда берутся данные, к которым она обращается. Напротив, если бы ядро целиком располагалось в физической памяти, то было бы очень трудно, например, выгружать на диск страницы таблицы страниц; если бы ядру было выделено отдельное адресное пространство, то перемещение данных между ядром и пользовательскими приложениями тоже усложнилось бы. А описанная конструкция (ныне широко используемая) позволяет приложениям рассматривать ядро почти как библиотеку, правда, защищенную.

Почему доступ к нулевому указателю вызывает ошибку сегментации показано на стр. 306.

В VMS впервые предложено два ставших стандартными приема: обнуление по запросу и копирование при записи. Опишем эти ленивые оптимизации.

Как и в других современных операционных системах и в VAX/VMS, адресное пространство в Linux состоит из пользовательской части (в ней располагаются код программы, стек, куча и другие разделы) и части ядра (код ядра, стеки, куча и другие вещи). Как и в других системах, при контекстном переключении изменяется только пользовательская часть адресного пространства текущего пр=роцесса, а ядерная остается без изменения. Как и в других системах, программа, работающая в режиме пользователя, не может обращаться к виртуальным страницам ядра; доступ к этой памяти возможен только в результате системного прерывания и после перехода в привилегированный режим.

Интересно, что в Linux есть два типа виртуальных адресов ядра. Во первых, это логические адреса ядра. Это то, что мы назвали бы нормальным виртуальным адресным пространством ядра; чтобы выделить память этого типа, код ядра должен вызвать функцию kmalloc.

Другой тип адресов называется виртуальными адресами ядра. Чтобы получить память этого типа, ядро вызывает функцию vmalloc, которая возвращает указатель на виртуально непрерывный участок памяти нужно размера.

На данный момент 64-битное адресное пространство используется не полностью, задействованы только младшие 48 бит.

Увеличение размера страницы важно не из за того, что уменьшает потребление памяти, сколько увеличивает производительность TLB.

У гигантских страниц есть своя цена. И главный потенциальный недостаток - внутренняя фрагментация, когда страница велика, но используется разреженно.

Пожалуй, самое существенное отличие современных систем ВП (Solaris, Linux и различные варианты BSD) и старых (VAX/VMS) - упор на безопасность.

Одна из основных угроз - атаки с переполнением буфера, которые можно применить как против обычных пользовательских программ, так и против самого ядра. Идея в том, чтобы найти в системе дефект, который позволит атакующему внедрить произвольные данные в ее адресное пространство.

Linux производит ленивое копирование страниц при записи после fork, уменьшая тем самым накладные расходы на ненужное копирование.

Все вышеперечисленное имеет место при ЛЮБОМ обращении к памяти.





II Конкурентность

ОС была первой многопоточной программой.

Введем новую абстракцию для одного работающего процесса: поток. На смену классическому представлению о существовании единственной точки выполнения внутри программы (т.е единственного счетчика команд) приходит идея многопоточной программы, в которой точек выполнения несколько (несколько регистров PС, указывающих на разные команды. Потоки можно представлять себе как отдельные процессы - с тем отличием, что они разделяют общее адресное пространство и, следовательно, могут обращаться к одним и тем же данным. У каждого потока есть свой набор регистров, используемых при вычислениях. Поэтому, если два потока работают на одном процессоре, то когда приостанавливается выполнение одного и начинается выполнение другого, должно происходить контектстное переключение. Этот процесс похож на контекстное переключение между процессами, когда состояние процесса сохранялось в блоке управления процессом (PCB), теперь же нам нужно несколько блоков управления потоком (thread control block - TCB), в которых будет сохраняться состояние отдельных потоков процесса. Но есть одно важное отличие между контекстным переключением процессов и потоков: адресное пространство остается тем же самым (т.е нет необходимости переключать таблицу страниц).

Еще одно отличие между процессами и потоками касается стека. В нашей простой модели АП классического процесса (однопоточного процесса) имеется всего один стек, который обычно располагается в конце адресного пространства. Однако в многопоточном процессе каждый поток работает независимо и для него существует отдельный стек. Таким образом, все выделенные в стеке переменные и т.д будут находиться в поточно-локальной памяти, т.е в стеке соответствующего потока. Обычно в этом нет проблем, потому что стеки редко бывают очень большими (если это не программы, в которых активно используется рекурсия).

Зачем нужны потоки? Есть, по крайней мере, две важные причины. Первая - параллелизм.

Задача преобразования стандартной однопоточной программы в программу, которая выполняет свою работу на нескольких CPU, называется распараллеливанием.

Вторая причина не столь очевидна - избежать приостановки программы из за медленного ввода-вывода и тем самым повысить степень использования оборудования.

Многопоточность открывает возможность перекрытия ввода вывода с другими действиями внутри одной программы, тогда как мультипрограммирование предназначено для той же цели, но между разными программами. Поэтому многие современные приложения (веб серверы, СУБД и т.д) используют потоки.

Конечно, во всех вышеупомянутых случаях можно было бы использовать несколько процессов вместо потоков. Но, поскольку потоки разделяют одно адресное пространство, становится проще использовать общие данные, так что при создании программ такого типа потоки - естественный выбор. Процессы разумнее использовать, когда имеются логически различные задачи, для которых общие структуры данных совсем или почти не нужны.

Созданный поток может сразу начать выполнение или же (в зависимости от настроения планировщика) в момент создания может оказаться в состоянии "готов", а не "выполняется". Конечно, в системе с несколькими процессорами потоки могли бы даже запуститься одновременно.

Для любой заданной последовательности команл существует много порядков выполнения, зависящих от того, какой поток выберет планировщик в разные моменты времени.

Нет никаких причин полагать, что поток, созданный раньше, раньше и начнет выполняться.

У каждого потока свой собственный набор регистров, регистры виртуализируются кодом контекстного переключения, который их сохраняет и восстанавливает.

Состояние гонки (точнее, гонки за данные) - это проблема многопоточных програм, при которой результат зависит от хронометража выполнения кода. Если нам не повезло (контекстное переключение имеет место в неудачные моменты времени), то результат будет неверен.

Поскольку выполнение этого кода несколькими потоками может приводить к состоянию гонки, он называется критической секцией. Это такой участок кода, в котором производится доступ к разделяемой переменной (или вообще к любому разделяемому ресурсу) и который должен конкурентно выполняться более одним потоком. В действительности нам хотелось бы, чтобы этот код обладал свойством взаимного исключения. Оно гарантирует, что если один поток находится в критической секции, то всем остальным вход в нее запрещен. Кстати, практически все эти термины были введены в оборот Эдсгером Дейкстрой, первопроходцем в этой области.

Один из способов решить проблему заключается в использовании более мощных команд, которые за один шаг делают все необходимое, так что прерывание просто не может возникнуть не вовремя.

Атомарное (неделимое) выполнение команды означение, что команда не может быть прервана посередите, ведь именно такую гарантию дает нам оборудование: если возникает прерывание, то либо команда не выполнена вовсе, либо выполнена до конца, никакого промежуточного состояния быть не может.

В этом контексте "атомарно" означает "как неделимое целое", или, по другому, "все или ничего".

В разумном наборе команд не существует "специфических" атомарных команд, типа "обновление конкурентного B дерева" и т.д. Это не практично. Поэтому вместо этого мы хотим, чтобы оборудование предоставляло несколько полезных команд, на основе которых мы могли бы построить общие примитивы синхронизации. С помощью аппаратных примитивов синхронизации в сочетании с кое какой помощью от операционной системы мы сумеет написать многопоточный код, который в критических секциях работает контролируемо.

Иногда объединение нескольких действий в одно атомарное называется транзакцией.

Продолжайте напрягать мозги, пока все не встанет на свои места.

Другой распространенный тип взаимодействия потоков: когда один поток должен ждать, пока другой завершит свои действия. Поэтому в последующих главах мы будем изучать не только построение примитивов синхронизации для поддержки атомарности, но и механизмы такого засыпания-пробуждения, часто применяемые в многопоточных программах.

Критическая секция - участок кода, в котором производится доступ к разделяемому ресурсу, обычно переменной или структуре данных.

Недетерминированная программа содержит одно или несколько состояний гонки.

[SR05] «Advanced Programming in the Unix Environment» by W. Richard Stevens
and Stephen A. Rago. Addison-Wesley, 2005. Как мы уже не раз говорили, купи-
те эту книгу и читайте ее. Понемножку, лучше перед сном. Во-первых, скорее
заснете, но важнее, что вы больше узнаете о том, как стать серьезным прог­
раммистом в Unix.

Структуры типа pthread_t используется для взаимодействия с потоком. Мы передаем указатель на неа в pthread_create, чтобы та ее инициализировала.

Вы можете спросить зачем нужны эти указатели на void? Да все просто: функции, принимающей указатель на void (в данном случае start_routine), можно передать агрумент любого типа, а функция, возвращающая такой указатель, может вернуть результат любого типа.

Для ожидания завершения потока нужно использовать функцию pthread_join.

После того, как поток завершился, функция pthread_join, которая ожидала его завершения, возвращает управление главному потоку, и тот может получить доступ к возвращенному значению.

Никогда не возвращайте указатель на что то, размещенное в стеке потока.

Функции, обеспечивающие взаимное исключение в критических секциях с помощью блокировок (самые главные): pthread_mutex_lock, pthread_mutex_unlock.

Блокировки весьма полезны, когда требуется защитить критическую секцию кода и тем самым гарантировать корректность операций.

Смысл блокировок следующий: если никакой другой поток не удерживает блокировку в момент вызова функции pthread_mutex_lock, то поток захватит блокировку и войдет в критическую секцию. Если же другой поток удерживает блокировку, то поток, желающий ее захватить, не вернет управление из этой функции, пока не получит блокировку в свое распоряжение. Разумеется, в каждый момент времени получения блокировки может ожидать много потоков, но только поток, захвативший блокировку, может вызвать unlock.

Каждая блокировка должны быть инициализированны, т.е начинать свою работу в правильном состоянии, иначе нельзя ожидать желаемого эффекта при блокировке и разблокировке. (PTHREAD_MITEXT_INITIALIZER, pthread_mutex_init).

После окончания работы с блокировкой нужно вызвать функцию pthread_mutex_destroy.

Еще один важный компонент любой библиотеки для работы с потоками и, конечно же, потоков POSIX - условные переменные. Они полезны, если нужно организовать механизм обмена сигналами между потоками, когда один поток может продолжить работу только после того, как другой что то сделает.

Основные функции из этого ряда: pthread_cond_wait, pthread_cond_signal.

Чтобы воспользоваться условной переменной, необходимо иметь блокировку, ассоциированную с данным условием. При вызове любой из показанных выше функций эта блокировка будет захватываться.

Тут надо сказать несколько слов. Во первых, в момент сигнализации (а также при модификации глобальной переменной ready) мы обязательно должны удерживать блокировку. Это дает уверенность, что мы по неосторожности не внесем в программу состояние гонки. Во вторых, вы, наверное, заметили, что функция wait принимает блокировку в качестве второго параметра, тогда как функция signal принимает только условную переменную. Это различие объясняется тем, что wait должна не только усыпить вызывающий поток, но и освободить при этом блокировку. Подумайте сами: ведь если бы она этого не сделала, то как бы другой поток смог захватить блокировку и просигнализировать о необходимости просыпаться? Однако перед тем как вернуть управление после пробуждения, функция pthread_cond_wait заново захватывает блокировку, гарантируя тем самым, что в любой момент времени между захватом блокировки в начале и освобождением ее в конце ожидающий поток работает, удерживая блокировку.

Не ленитесь, используйте условные переменные, даже когда вам кажется, что без них можно обойтись.

Для написания надежного и эффективного многопоточного кода больше почти ничего и не нужно - только терпение и аккуратность.

Сложность при работе с потоками представляет не API, а продумывание логики конкурентной программы.

Программисты вставляют блокировки в исходный код, окружая ими критические секции, и таким образом гарантируют, что код в критической секции выполняется как одна атомарная команда.

Блокировка - это просто переменная, поэтому мы должны объявить переменную блокировки какого-то типа. В переменной блокирвки (или просто блокировке) хранится текущее состояние блокировки. Она либо свободна (разблокирована, доступна), т.е ни один поток не удерживает ее, либо захвачена (заблокирована, занята), т.е ее удерживает ОДИН И ТОЛЬКО ОДИН поток, вероятно, находящийся в критической секции. В переменной можно хранить и другую информацию, например какой поток удерживает блокировку или очередь потоков, выстроившуюся к блокировке), но она скрыта от пользователя блокировки.

Вызов lock пытается захватить блокировку; если никакой другой поток ее не удерживает (блокировка свободна), то поток захватит ее и войдет в критическую секцию (станет владельцем блокировки). Если какой то поток впоследствии вызовет lock для той же самой переменной блокировки, то вызов не вернет управление, пока блокировка удерживается другим потоком. Таким образом, другие потоки не могут войти в критическую секцию, пока в ней находится владелец блокировки. После того как владелец блокировки вызовет unlock, блокировка снова станет доступной. Если другие потоки не ждут ее (т.е ни один поток не вызвал ранее lock и не оказался заблокированным), то состояние блокировки просто изменяется на "свободна". Если же имеются ожидающие потоки, то один из них будет (в конечном итоге) проинформирован об изменении состояния блокировки, захватит блокировку и войдет в критическую секцию.

Блокировки дают программисту минимальный контроль над планированием.

Блокировка превращает хаос, характерный для традиционного планирования ОС, в более упорядоченную последовательность.

В стандарте POSIX для блокировок употребляется название мьютекс, поскольку они применяются для обеспечения взаимного исключения.

Вы наверное заметили, что в версии POSIX функциям блокировки и разблокировки передается переменная, потому что для защиты разных секций можно использовать разные блокировки. Это позволяет повысить степень конкурентности: вместо того, чтобы использовать одну большую блокировку для доступа к любой критической секции (крупная блокировка), мы часто защищаем разные структуры данных различными блокировками, допуская исполнение нуждающегося в защите кода сразу несколькими потоками  (мелкие блокировки).

Для построения работоспособной блокировки нам понадобиться помощь от нашего старого друга, оборудования, и от нашего доброго приятеля, ОС.

При проектировании блокировки она прежде всего должна отвечать своему главному предназначению - обеспечивать взаимное исключение.

Один из критериев оценки эффективности блокировки - справедливость. Получают ли все потоки, конкурирующие за блокировку, равные шансы захватить освободившуюся блокировку?

Одним из первых решений проблемы взаимного исключения стало запрещение прерываний в критических секциях, это решение было предназначено для однопроцессорных систем (запрет на прерывание от таймера). Основное достоинство этого подхода - простота. В отсутствие прерываний поток может быть уверен, что никакой другой поток не нарушит ход выполнения кода. А вот недостатков просто дохуя. Во первых, требуется, чтобы любому вызвающему потоку было разрешено выполнить привилегированную операцию, а значит, мы вынуждены доверять исполняемому в этом потоке коду (роем себе яму...). Запрещение прерываний как общий метод синхронизации озночало бы, что мы слишком доверяем приложениям. Во вторых, этот подход не работает в системах с несколькими процессорами. (ведь прерывания запрещаются для одного исполняемого ядра). В третьих, если запретить прерывания на длительное время, то часть прерываний может быть потеряна, что ведет к серьезным общесистемным проблемам. Наконец, такое решение может оказаться неэффективным. По всем этим причинам запрещение прерываний в качестве примитива взаимного исключения используется только в ограниченных контекстах. Например, в некоторых случаях ОС запрещает прерывания для гарантии атомарности при доступе к собственным структурам данных и чтобы предотвратить возможный хаос при обработке прерываний. Такое использование имеет смысл, потому что внутри ОС проблемы доверия нет - она в любом случае доверяет самой себе хотя бы для того, чтобы выполнять привилегированные операции. 

"примитив" означает базовый, простейший механизм или инструмент, который используется для решения конкретной задачи. Это не означает, что он "простой" в смысле "примитивный" (как в бытовом значении), а скорее то, что это фундаментальный строительный блок для более сложных систем.
Почему используется слово "примитив"?
В программировании и компьютерных науках "примитив" — это элементарная конструкция, которая предоставляет базовую функциональность.

Потерпев неудачу с методами на основе прерываний, мы вынуждены обратиться за поддержкой к оборудованию и предоставляемым им командам.

Проблемы простой реализованной блокировки на основе обычного флага: корректность и производительность.

Проблема производительности заключается в том, как именно поток ожидает захвата уже удерживаемой блокировки: он бесконечно проверяет значение флага, т.е занимается активным ожиданием (spin-waiting). Активное ожидание - бессмысленное сжигание процессорного времени в ожидании освобождения блокировки другим потоком. Накладные расходы особенно высоки на машине с одним процессором, поскольку поток, которого мы с таким нетерпением ждем, даже не может работать. Поэтому в процессе разработки более подходящих решений мы должны избегать такого рода потерь.

Поскольку при наличии нескольких процессоров запрещение прерываний не работает, как и простые подходы на основе команд загрузки и сохранения, конструкторы систем принялись изобретать аппаратную поддержку блокировки.

Простейший для понимания элемент аппаратной поддержки - команда проверки и установки (test-and-set), или команда атомарного обмена. Этой чуть более мощной команды уже достаточно для построения простой спин-блокировки (блокировки с активным ожиданием).

Сделав проверку (старого значения блокировки) и установку (нового значения) одной атомарной операцией, мы гарантировали, что только один поток может ахватить блокировку. И стало быть, создали работающий примитив взаимного исключения. Для правильной работы спин-блокировок на одном процессоре необходим вытесняющий планаровщик (который периодически прерывает поток по таймеру). Без вытеснения спин-блокировки на одном процессоре были бы бессмысленны, потому что крутящийся в цикле поток никогда не уступил бы CPU.

Еще один аппаратный примитив, предоставляемый некоторыми системами, - команда сравнить и обменять (в архитектуре SPARC она называется compare-and-swap, а в архитектуре х86 - compare-and-exchange)

Идея команды "сравнить и обменять" заключается в том, чтобы проверить, равно ли значение по адресу ожидаемому. Если да, то в память по этому адресу записывается новое значение, а если нет, то ничего и не делается.

Имея эту команду, мы можем построить блокировку примерно так же, как с помощью команды "проверить и установить".

Команда "сравнить и обменять" мощнее, чем "проверить и установить".

И последний аппаратный примитив - команда "выбрать и прибавить" (fetch-and-add), которая атомарно увеличивает значение по указанному адресу и возвращает старое.

Эту команду можно использовать для интересной билетной блокировки (ticket lock)

Отличие билетной блокировкт от предыдущих решение: оно гарантирует продвижение всем потокам, т.е является справедливой блокировкой. Предыдущие попытки такой гарантии не давали: поток, пытающийся выполнить команду проверки и установки (например), мог крутиться в цикле вечно, безутешно наблюдая за тем, как другие потоки захватывают и освобождают блокировку.

Проблема всех приведенных выше решений - активное ожидание, которое в некоторых случаях может быть очень неэффективно (например, один процессор). В этом случае ожидающий блокировки процесс должен потратить весь свой временной квант для прокрутки цикла, брррр....

Как разработать блокировку, которая не будет бесполезно тратить время, крутясь в цикле? Одной аппаратной поддержки для решения этой проблемы недостаточно. Необходима еще и поддержка со стороны ОС.

Аппаратная поддержка позволила продвинуться довольно далеко: мы имеем работоспособные блокировки и даже (как показывает билетная блокировка) справедливое отношение к конкурирующим потокам. Но как быть с активным ожиданием?

Наша первая попытка проста и дружелюбна: если собираешься перейти в режим активного ожидания, лучше сразу уступи процессор другому потоку (yield). Предполагается, что ОС предоставляет примитив yield. Этот системный вызов просто переводит вызывающий поток из состояния выполняется в состояние готов. Таким образом, уступающий процесс сознательно отказывается от процессора.

Но даже так, при наличии большого количества процессов, стоимость контекстного переключения дорога и напрасно будет израсходовано много времени.

Хуже того, не была решенена проблема зависания (справедливости).

Проблема предыдущих подходов заключается в том, что они слишком многое оставляют на волю слепого случая. Поэтому нам нужен явный кнтроль над тем, какой поток получит блокировку, после того как текущий владелец освободит ее. Для этого потребуется чуть больше поддержки со стороны ОС, а также очередь для хранения потоков, ожидающих возможности захватить блокировку.

Системный вызов gettid возвращает идентификатор текущего потока.

Проснувшись поток выглядит так, как будто он вернулся из park.

Еще одна причина избегать активного ожидания - инверсия приоритетов. Есть несколько подходов к решению этой проблемы. В частном случае, когда причиной являются спин блокировки, можно отказаться от них. А в общем случае высокоприоритетный поток, ожидаюзий низкоприоритетного, может временно повысить приоритет последнего, дав ему возможность поработать и тем самым преодолеть инверсию. Эта техника называется наследованием приоритетов. И последнее решение самое простое - назначать всем потокам одинаковые приоритеты.

До сих пор мы видели только один тип поддержки, оказываемой ОС с целью повысить эффективность блокировки в библиотеке для работы с потоками (уступки, засыпание потока). Другие ОС предлагают аналогичную поддержку, но детали могут варьироваться. Например, в Linux имеются фьютексы - интерфейс, аналогичный Solaris, но с большей функциональностью внутри ядра. Именно, с каждым фьютексом ассоциированна ячейка в физической памяти и расположенная внутри ядра очередь. Поток вызывает функции фьютекса для засыпания и пробуждения.

И последнее замечание: реализованное в Linux решение несет в себе дух старого подхода, который на протяжении многих лет то принимался, то отвергался. Он известен, по крайней мере, со времен блокировое Дэма в начале 1960-ых годов и ныне называется двухфазной блокировкой. Этот подход признает, что активное ожидание может быть полезным, особенно если блокировку вот вот освободят. Поэтому на первом этапе функция активно ждет, правда очень недолго, в надежде, что удастся захватить блокировку. Но если блокировку не удалось захватить на первом этапе, то наступает второй этап, на котором вызывающий поток засыпает и пробуждается, только когда блокировка освободиться. Показанная выше блокировка в Linux - разновидность такого подхода всего с одной итерацией цикла ожидания; обобщением был бы цикл, расчитанный на фикс. время, по истечении которого средства фьютекса используются для засыпания. Двухфазная блокировка - еще один пример гибридного подхода, когда сочетание двух хороших идей может дать отличный результат.

Мы описали, как в наши дни конструируются реальные блокировки: нужна поддержка со стороны оборудования (в форме специальных команд) и со стороны операционной системы (например, в форме примитивой park и unpark в Solaris или фьютексов в Linux).

Блокировки добавляются в структуру данных, чтобы ее можно было использовать в потоках, благодаря этому структура становиться потокобезопасной. Разумеется, корректность и производительность структуры данных зависят от того, как именно добавлены блокировки.

Одновременно, т.е конкурентно.

Если структура данных работает слишком медленно, то, возможно, добавлением всего одной блокировки ограничиться не удастся. Отметим, что если структура данных работает достаточно быстро, то больше ничего делать не нужно. Ни к чему изобретать хитроумный механизм, если хватает и простого.

В идеале мы хотели бы, чтобы несколько потоков завершалось на нескольких процессорах так же быстро, как один поток на одном. Достижение этой цели называется идеальным масштабированием.

Для создания масштабируемых счетчиков можно применить подход приближенный счетчик. В этом случае один логический счетчик представляется несколькими локальными физическими счетчиками, по одному для каждого процессорного ядра, а также одним глобальным счетчиком. Конкретно, на машине с четырьмя CPU будет четыре логических и один глобальный счетчик. Помимо этих счетчиков есть еще блокировки: по одной для каждого локального счетчика (в предположении, что на каждом ядре работает несколько потоков) и еще одна для глобального.

Именно компромисс между точностью и производительностью и лежит в основе приближенного счетчика.

....Что касается функции поиска, то мы проделали простое преобразование кода, чтобы после выхода из цикла поиска попасть на общий путь выхода. Это уменьшает количество точке захвата и освобождения блокировок в коде, а значит, и шансы случайно внести ошибку.

Для повышения масштабируемости списка была изучена техника блокировки вперехват (или цепной блокировки). Идея довольно проста. Нужно завести не одну блокировку на весь список, а по одной на каждый узел. В процессе обхода списка мы сначала захватываем блокировку следующего узла, а затем освобождаем блокировку текущего.

Если придуманная вами схема привносит большие накладные расходы (например, захватывает и освобождает блокировку не один раз, а многократно), то, возможно, повышение степени конкурентности окажется не столь важным.

Как вы уже знаете, всегда есть стандартный способ сделать структуру данных конкурентной: добавить большую блокировку.

Совет: избегайте преждевременной оптимизации (закон Кнута). Проектируя конкурентную структуру данных, начните с самого простого подхода, т.е заведите одну большую блокировку. Это, скорее всего, позволит получить правильное решение, а если обнаружится, что оно недостаточно производительное, то можно будет уточнить его. Структура долюна быть настолько быстрой, насколько необходимо, а добиваться большего не имеет смысла. "Преждевременная оптимизация - корень всех зол" by Кнут.

На первых этапах перехода на мультипроцессоры в Linux и других ОС использовалась единственная блокировка. В Linux у нее даже было название: большая блокировка ядра (BKL). В начале это решение было хорошим, но позже стало узким местом.

Таким образом, по ходу дела мы извлекли несколько важных уроков: что при изменении потока управления нужно внимательно следить за захватом и освобождением блокировок; что повышение степени конкурентности необязательно означает повышение производительности; что проблемами производительности нужно заниматься, только когда они действительно существуют. Последний пункт - избегать преждевременной оптимизации - должен накрепко запомнить любой разработчик, озабоченный производительностью: ни к чему тратить силы на ускорение какой то операции, если при этом общая производительность приложения не повыситься.

Блокировки не единственные примитивы, необходимые для построения конкурентных программ. В частности, есть много случаев, когда поток хочет проверить, выполняется ли какое-то условие, прежде чем продолжить выполнение.

Можно было бы воспользоваться разделяемой переменной. Вообще говоря, такое решение работает, но чудовищно неэффективно, потому что родитель крутиться в цикле и напрасно транжирит процессорное время. Хотелось бы, чтобы вместо этого родитель спал, пока не окажется выполненным условие, которого мы ждем.

Чтобы дождаться выполнения условия, поток должен воспользоваться условной переменной. По существу, это явная очередь, в которую поток может поместить себя, если в данный момент некоторое условие ложно и требуется ждтаь, когда оно станет истинным. Другой поток, изменив это условие, может пробудить один или несколько ожидающих его потоков, дав им возможности продолжить выполнение (или, как говорят, сигнализировав условию).

Для условных переменных определены две операции: wait и signal. Первая вызывается, когда поток хоччет заснуть, а вторая - когда поток что то изменил в состоянии программы и хочет пробудить спящий поток, ожидающий этого условия. В стандарте POSIX эти функции объявлены следующим образом: pthread_cond_wait(cond_t, mutex_t), pthread_cond_signal(cond_t).

Обратите внимание, что ^^ wait принимает мьютекс в качестве параметра и предполагает, что в момент вызова этот мьютекс захвачен. На wait возлагается обязанность освободить блокировку и усыпить вызвавший поток (атомарно), а когда поток пробудиться (после сигнала из другого потока), он должен заново захватить блокировку, перед тем как вернуть управление вызывающей стороне. Смысл этой процедуры в том, чтобы предотвратить состояния гонки, возможные, когда поток засыпает.

Совет: сигнализируя, всегда удерживайте блокировку. Удерживать блокировку в момент сигнализации условной переменной, наверное, проще и лучше всего, пусть даже и не всегда это необходимо. В примере выше показан случай, когда блокировку удерживать обязательно, иначе программа окажется некорректной. Бывают случаи, когда этого можно и не делать, но надежнее всегда удерживать блокировку в момент сигнализации.
Обратная сторона этого совета - удерживать блокировку при вызове wait - это и не совет вовсе, а обязательная семантика ожидания, которая предполагает, (а) что в момент вызова блокировка захвачена, (б) что блокировка освобождается перед усыплением процесса и (с) что блокировка повторно захватывается перед возвратом управления. Таким образом, совет можно представить в обобщенной форме: удерживайте блокировку в момент сигнализации и ожидания, и все у вас будет ХОРОШО.

Как только поток пробуждается ото сна в ожидании условной переменной, он помещается в очередь готовых к исполнению потоков; теперь он готов к выполенению (но еще не выполняется).

Причина проблемы проста: после того как производитель разбудил Tc1, но ДО того как Tc1 начал работать, состояние ограниченного буфера изменилось (из за Tc2). Полученный сигнал только пробуждает поток, это не более чем предположение о том, что состояние мира могло измениться (в данном случае в буфер было помещено значение), но не ГАРАНТИЯ того, что когда разбуженный поток начнет работать, состояние все еще будет таким, как ему хотелось бы. Эту интерпретацию сигнала часто называют семантикой Mesa. Семантику Хоара реализовать труднее, но она дает более сильную гарантию, поскольку разбуженный поток начинает работать сразу после пробуждения. Практически во всех когда либо созданных системах используется семнатика Mesa.

Благодаря семантике Mesa при работе с условными переменными действует простое правило: всегда использовать циклы while.

pthread_cond_signal пробуждает только один из ожидающих потоков.

В задаче потребителей и производителей потребитель должен будить не потребителей, а ТОЛЬКО производителей - а наоборот. При наличии единственной условной переменной это не всегда так, так как и потребители и производители помещаются в одну и ту же очередь потоков, что может привести к вечному сну всех потоков.

Решение в этом случае простое: использовать не одну, а две условне переменные, чтобы можно было правильно указать, какого типа потоки будить при изменении состояния системы.

Здесь потоки производители ждут по условию empty, а сигнализируют условию fill. Наоборот, потоки потребители ждут по условию fill, а сигнализируют empty

Для проверки условия в многопоточной программе использовать цикл while всегда правильно и безопасно, а корректность применения if зависит от семантики сигнализации. Поэтому используйте while - и ваш код будет работать, как задумано. Обертывание проверки циклом while также решает проблему ложного пробуждения. В некоторых библиотеках для работы с потоками из за детали реализации возможно пробуждение сразу двух потоков при поступлении всего одного сигнала. Ложное пробуждение - еще одна причина перепроверить условие, которого ждет поток.

pthread_cond_broadcast будит все ожидающие условия потоки. Лэмпсон и Рэделл назвали такое условие покрывающим, поскольку оно покрывает все случаи, когда поток должен проснуться.

Широковещательный сигнал - сигнал при использовании pthread_cond_broadcast.

В общем случае, если оказывается, что ваша программа работает, только если заменить направленный сигнал широковещательным (хотя вам не кажется, что такдолжно быть), вероятно, в ней есть ошибка: исправьте ее!

Мы познакомились еще с одним важным примитивом синхронизации, отличным от блокировок: условными переменными. Они позволяют потокам спать, пока не будет выполнено интересующее их условие, и тем самым решают целый ряд важных проблем синхронизации.

Эдсгер Дейкстра первый описал такой примитив синхронизации, как семафор. На самом деле Дейкстра с сотрудниками предложили семафор как единственный примитив синхронизации для всего связанного с синхронизацией.

Семафор - это объект, принимающий целочисленное значение, которым можно манипулировать с помощью двух операций, в стандарте POSIX они называются sem_wait и sem_post. Поскольку начальное значение самафора определяет его поведение, перед вызовом любой операции необходимо инициализировать семафор (POSIX - sem_init).

Двоичные семафоры могут выступать в роли блокировок. Для этого начальное значение семафора должно быть равно 1. Поскольку блокировка может находиться только в двух состояниях, иногда, семафор, используемый в таком качестве, называют двоичным. 

Семафоры полезны также для упорядочения событий в конкурентной программе. Таким образом, мы используем семафор как примитив упорядочения (что близко к описанному выше использованию условных переменных).

Повысить степень конкурентности значит сделать так, чтобы в каждый момент времени работало большее число потоков.

При реализации блокировки чтения записи очень важно следить за справедливостью решения. Например, большое количество читателей могут просто приватизировать процессор, оставляя писателя за собой. Это можно исправить, например, ограничивая добавления читателей после инициирования желания писателя внести изменения в список.

Никогда не нужно недооценивать идею о том, что незамысловатый подход может оказаться самым лучшим (закон Хилла).

Семафоры - мощный и гибкий примитив для написания конкурентных программ. Некоторые программисты только им и пользуются, сторонясь блокировок и условных переменных, поскольку ценят их за простоту и полезность.

Среди изученных Лю и др. ошибок, не связанных с взаимоблокировкой, значительная доля (97%) - нарушение атомарности и порядка. Поэтому, помня об этих видах ошибок, программисты имеют больше шансов избежать их.

Основные типы ошибок этого класса: нарушение атомарности и нарушение порядка.

Если порядок потоков имеет значение, то на помощь приходят условные переменные (или семафоры).

Вы наверное думаете что таких простых взаимоблокировок, как показанная выше, избежать легко. Например, если потоки 1 и 2 всегда будут захватывать блокировки в одном и том же порядке, то взаимоблокировка никогда не случиться. Так почему же они возникают? Одна из причин заключается в том, что в больших кодовых базах между компонентами имеются сложные зависимости. Поэтому в больших системах проектировать стратегии блокировки нужно очень внимательно, чтобы избежать взаимоблокировок из за циклическиз зависимостей, естественно возникающих в коде. Еще одна причина - сама природа инкапсуляции (сокрытие реализации для упрощения построения модульных программ). Однако модульность плохо ухаживается с блокировкой.

Для возникновения взаимоблокировки необходимо выполнение четырех условий:
1) взаимное исключение
2) ожидание с удержанием
3) отсутствие вытеснения
4) циклическое ожидание
Если хотя бы одно из этих 4ех условий не выполняется, взаимоблокировки не случится.

Пожалуй, самым практически полезным методом предотвращения взаимоблокировок является написание кода захвата блокировок таким образом, чтобы циклическое ожидание не возникало никогда. И проще всего это сделать, определив полное упорядочение захвата блокировок. Разумеется, в более сложных системах количество блокировок больше, поэтому обеспечить их полное упорядочение трудно (да, может быть, и не нужно). Поэтому для избегания взаимоблокировок полезно частичное упорядочение захвата.

Есть вариант упорядочивать блокировки по адресу перед захватом для гарантии одинакового порядка захвата. Эта техника дает простую эффективную реализацию захвата нескольких блокировок без риска взаимоблокировки.

Избежать ожидания с удержанием можно, захватывая все блокировки разом - атомарно. При таком подходе может снизиться степень конкурентности, потому что блокировки захватываются предварительно (и все сразу), а не тогда, когда они действительно необходимы. Также, как и раньше, мы страдаем от инкапсуляции: при вызове функции мы должны точно знать, какие блокировки нужно удерживать, и захватить их заранее.

Последний метод предотвращения - вообще избежать взаимного исключения. Херлихи придумал, как можно спроектировать различные структуры данных вообще без блокировок. Идея безблокировочных подходов (и родственных им подходов без ожидания) проста: воспользовавшись мощными аппаратными командами, построить структуры данных способом, при котором явные блокировки не нужны.

Вместо предотвращения взаимоблокировок в некоторых случаях случше их избегать. Для этого нужно иметь глобальную информацию о том, какие блокировки могут затребовать различные потоки в ходе своего выполнения, и планировать эти потоки таким образом, чтобы взаимоблокировка гарантированно не возникла. К сожалению, такой подход полезен только в очень специфических обстоятельствах, например во встраиваемой системе, когда весь набор задач и необходимых им блокировок известен заранее. Кроме того, эти подходы ограничивают степень конкурентности. Поэтому избегание взаимоблокировок посредством планирования нельзя назвать широко распространенным универсальным решением.

"Не все достойное быть сделанным достойно быть сделано хорошо" (Том Уэст).

И последняя наша стратегия заключается в том, чтобы смириться с возможностью редких взаимоблокировок, а меры предпринимать после того, как эта неприятность обнаружена.

Во многих системах баз данных применяются методы обнаружения взаимоблокировок и восстановления после них. Детектор взаимоблокировок запускается периодически, строит граф ресурсов и ищет в нем циклы. Если цикл существует (взаимоблокировка), то систему нужно перезапустить.

Нет ничего хуже преждевременной оптимизации программы.

Читай, читай и еще раз читай! А потом эксперементируй, напиши код, а затем напиши еще.

Процессор подключен к основной памяти с помощью шины памяти. К системе также подключены устройства с помозью шины ввода-вывода; во многих современных системах в этом качестве цели используется шина PCI. Среди этих устройств могут быть графические карты и другие высокоскоростные устройства ввода-вывода. Наконец, еще ниже расположена одна или несколько периферийных шин, например SCSI, SATA или USB. С их помощью к системе подключаются сравнительно медленные устройства: диски, мыши и клавиатуры.

Рассмотрим каноническое устройство (не настоящее) и на его примере постараемся разобраться в механизмах, которые необходимы для эффективного взаимодействия с устройством.

Устройство состоит из двух основных компонентов. Первый - аппаратный интерфейс, предоставляемый остальным частям системы. Как и программы, оборудование должно предоставлять какой то интерфейс, чтобы программные компоненты могли управлять его работой. Поэтому все устройтсва имеют четко определенный интерфейс и протокол взаимодействия.

Второй компонент любого устройства - его внутренняя структура. Она зависит от реализации и отвечает за реализацию абстракции, которую устройство предлагает системе.

Прошивка - программное обеспечение внутри аппаратного изделия, которое реализует его функциональность.

Базовый протокол хорош тем, что прост и работает. Но в некоторых отношениях он неудобен и неэффективен. Первая проблема - неэффективность опроса. Как это можно исправить? Инженеры много лет назад придумали, как улучшить это взаимодействие; это механизм прерываний. Вместо того чтобы опрашивать устройство в цикле, ОС выдает команду, усыпляет вызывающий процесс и переключает контекст, переходя к следующей задаче. Когда устройство завершит процесс, оно возбудит аппаратное прерывание, в результате чего CPU перейдет по предопределенному адресу программы обработки прерывания, или, проще, обработчика прерывания. Обработчик - это часть ОС, которая завершает выполнение запроса (например, читает данные и, возможно, код ошибки, возвращенный устройством) и будит процесс, ожидающий результата ввода вывода. Таким образом прерывания открывают возможность перекрытия вычислений и ввода-вывода, что является ключом к эффективному использованию процессора.
Если в перемещении данных участвует главный CPU (как в примере протокола), то программируемом вводе-выводе.

Отметим, что прерывания не всегда являются наилучшим решением. Например, представьте устройство, которое выполняет операции очень быстро, так что уже на первой итерации цикла опроса процессор обнаруживает, что устройство все сделало. В таком случае прерывание лишь замедлило бы работу системы, поскольку переключение на другой процесс, обработка прерывания и возврат к исходному процессу - все это дорогостоящие действия. Поэтому быстрое устройство лучше опрашивать, а при работе с медленым использовать прерывания. Если скорость устройства неизвестна или иногда оно работает медленно, а иногда быстро, то оптимальным будет гибридное решение: сначала несколько раз опросить, а если оно занято, перейти в режим прерывния. Такой двухфазный подход позволяет взять лучшее из обоих миров.

Использовать прерывания имеет смысл только для относительно медленных устройств. В противном случае стоимость обработки прерываний и контекстного переключения может перевесить выгоду.

Еще одна причина не использовать прерывания свойственна сетям. Если каждый из огромного потока входящих пакетов будет генерировать прерывание, то ОС может оказаться в ситуации активной блокировки, когда занята только обработкой прерываний и не дает пользовательским процессам возможности поработать и обслужить запросы. В таком случае лучше время от времени использовать опрос для лучшего контроля над происходящим в системе.

Существует также оптимизация, называемая объединением прерываний. В этом случае устройство, прежде чем генерировать прер., немного ждет. Во время ожидания могут завершиться другие запросы, и тогда устройство сможет доставить сразу несколько прерываний, объединенных в одно, снизив тем самым накладные расходы на обработку прерываний.

PIO — Программный ввод-вывод (англ. Programmed input/output, PIO), метод передачи данных между двумя устройствами, использующий процессор как часть маршрута данных (процессор выполняет команду чтения порта, считывает байт или слово данных в свой регистр, после чего переписывает его в память, затем повторяет эту процедуру до тех пор, пока вся необходимая информация не будет считана из устройства в память).

При использовании PIO CPU тратит слишком много времени на перемещение данных на устройство и от него. Как можно снять с процессора эту нагрузку и тем саым повысить эффективность его использования? Эта проблема решается с помощью технологии прямого доступа к памяти (ПДП, DMA). Контроллер ПДП - это очень специфическое устройство в системе, которое координирует передачу данных между устройствами и основной памятью почти без участия CPU. Он работает следующим образом. Чтобы передать данные устройству, ОС программирует этот контроллер, сообщая ему, где в памяти находятся данные, сколько данных копировать и какому устройству их передать. На этом участие ОС в передаче заканчивается, она может заняться другой работой. По завершении передачи контроллер генерирует прерывание, сообщая тем самым ОС, что работа выполнена.

Со временем развития получили два основных способа взаимодействия с устройтсвами. Первый, самый старый - явные команды ввода-вывода. Чтобы отправить данные устройству, программа указывает, в каком регистре находятся данные, а также порт, идентифицирующий устройство.

Второй способ взаимодействия с устройствами называется ввод-вывод с отображением памяти. При таком подходе оборудование представляет регистры устройтсва так, будто они являются ячейками памяти. Для доступа к конкретному регистру ОС выполняет команду загрузки, а оборудование переадресует команды устройтсву вместо основной памяти.

Как сделать ОС большую часть ОС независимой от устройств и тем самым скрыть детали взаимодействия с устройствами от основных подсистем ОС? Проблема решается применением старой как мир техники абстрагирования. На самом низком уровне существует какая то часть ОС, которая знает обо всех деталях работы устройтсва. Эта часть называется драйвером устройства, именно в ней инкапсулирована вся специфика взаимодействия с устройством.

Прерывания, ПДП и смежные идеи - все это прямые следствия того, что процессор работает быстро, а устройства медленно; елси бы вы жили в том время, то, наверное, пришли бы к таким же идеям.


